{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大似然估计原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果回归模型存在非线性，可能不方便使用 OLS，这时常常采用最大似然估计法（MLE）或非线性最小二乘法（NLS，参见第25章）。\n",
    "\n",
    "假设随机向量 $y$ 的概率密度函数为 $f(y; \\theta)$，其中 $\\theta$ 为 $K$ 维未知参数向量，通过抽取随机样本 ${y_1,\\cdots,y_n}$ 来估计 $\\theta$。假设${y_1,\\cdots,y_n}$ 为独立同分布（iid），则样本数据的联合密度函数为 $f(y_1; \\theta)f(y_2; \\theta) \\cdots f(y_n; \\theta)$。\n",
    "\n",
    "在抽样之前，${y_1,\\cdots,y_n}$ 被视为随机向量。抽样之后，${y_1,\\cdots,y_n}$ 就有了特定的样本值。因此，可以将样本的联合密度函数视为在 ${y_1,\\cdots,y_n}$ 给定的情况下，未知参数 $\\theta$ 的函数。定义“似然函数”（likelihood function）为\n",
    "$$L\\left(\\boldsymbol{\\theta} ; y_{1}, \\cdots, y_{n}\\right)=\\prod_{i=1}^{n} f\\left(y_{i} ; \\boldsymbol{\\theta}\\right)$$\n",
    "\n",
    "由此可知，似然函数与联合密度函数完全相等，只是 $\\theta$ 与 ${y_1,\\cdots,y_n}$ 的角色互换，即把 $\\theta$ 作为自变量，而视 ${y_1,\\cdots,y_n}$ 为已给定的。为了运算方便，常把似然函数取对数，将乘积的形式转化为求和的形式\n",
    "$$\\ln L\\left(\\boldsymbol{\\theta} ; y_{1}, \\cdots, y_{n}\\right)=\\sum_{i=1}^{n} \\ln f\\left(y_{i} ; \\boldsymbol{\\theta}\\right)$$\n",
    "\n",
    "“最大似然估计法\"（Maximum Likelihood Estimation，简记 MLE 或 ML）来源于一个简单而深刻的思想：给定样本取值后，该样本最有可能来自参数 $\\theta$ 为何值的总体。换言之，寻找 $\\hat{\\boldsymbol{\\theta}}_{\\mathrm{ML}}$，使得观测到样本数据的可能性最大，即最大化对数似然函数（loglikelihoodfunction）：\n",
    "$$\\max _{\\theta \\in \\Theta} \\ln L(\\theta ; y)$$\n",
    "\n",
    "在数学上，常把最大似然估计量 $\\hat{\\boldsymbol{\\theta}}_{\\mathrm{ML}}$ 写为\n",
    "$$\\hat{\\boldsymbol{\\theta}}_{\\mathrm{ML}} \\equiv \\operatorname{argmax} \\ln L(\\boldsymbol{\\theta} ; y)$$\n",
    "\n",
    "其中，“argmax\"（即argument of the maximum）表示能使 $ln L(y; \\theta)$ 最大化的 $\\theta$ 取值。假设存在唯一内点解，则该无约束极值问题的一阶条件为\n",
    "$$s(\\boldsymbol{\\theta} ; y) \\equiv \\frac{\\partial \\ln L(\\boldsymbol{\\theta} ; y)}{\\partial \\boldsymbol{\\theta}} \\equiv\\left(\\begin{array}{c}\n",
    "\\frac{\\partial \\ln L(\\boldsymbol{\\theta} ; y)}{\\partial \\theta_{1}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\ln L(\\boldsymbol{\\theta} ; y)}{\\partial \\theta_{K}}\n",
    "\\end{array}\\right)=\\mathbf{0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodels 库实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matlab实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 OLS 估计代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```matlab\n",
    "function [ARy,ARx] = ARxx(X,P)\n",
    "\n",
    "% 获取自回归的估计向量\n",
    "%  X   - 输入数据，列向量\n",
    "%  P   - AR 阶数,标量\n",
    "\n",
    "N = length(X);\n",
    "ARx = zeros(N-P,P+1);\n",
    "ARx(:,1) = ones(N-P,1);   %常数项\n",
    "for m = 1:P\n",
    "    ARx(:,m+1) = X(P-m+1:N-m,1);  %P-m+1 = N-m-(N-P)+1\n",
    "end\n",
    "ARy = X(P+1:N,1);\n",
    "\n",
    "% % 1.载入数据\n",
    "% data = xlsread('C:\\Users\\Administrator\\Desktop\\hourse.xlsx');\n",
    "% f1 = data(:,2); f2 = data(:,3); e = data(:,6);\n",
    "% \n",
    "% % 2.估计自回归模型f1 = c + f1(-1) + f1(-2)\n",
    "% [ARy,ARx] = ARxx(f1,2);\n",
    "% beta = regress(ARy,ARx);\n",
    "% \n",
    "% % 3.计算各滞后阶AIC值,选取最优滞后阶数(最大阶数为6)\n",
    "% T = length(f1);\n",
    "% AR_AIC = zeros(6,1);\n",
    "% for p = 1:6\n",
    "%     [ARy,ARx] = ARxx(f1,p);\n",
    "%     beta = regress(ARy,ARx);\n",
    "%     y_hat = ARx*beta;\n",
    "%     resid = ARy - y_hat;\n",
    "%     AR_AIC(p) = log(resid'*resid/T)+2*(p+1)/T;\n",
    "% end\n",
    "% \n",
    "% % 4.找到最小的AIC值所对应的阶数\n",
    "% Best_p = find(AR_AIC == min(AR_AIC));\n",
    "% [ARy,ARx] = ARxx(f1,Best_p);\n",
    "% beta = regress(ARy,ARx);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fminsearch 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fminsearch 函数用来求解多维无约束的线性优化问题，用 derivative-free 的方法找到多变量无约束函数的最小值\n",
    "\n",
    "**语法：**\n",
    "```matlab\n",
    "x = fminsearch(fun,x0)   \n",
    "     x = fminsearch(fun,x0,options)    \n",
    "     [x,fval] = fminsearch(...)\n",
    "     [x,fval,exitflag] = fminsearch(...)\n",
    "     [x,fval,exitflag,output] = fminsearch(...)\n",
    "```\n",
    "\n",
    "**解释：**\n",
    "\n",
    "fminsearch能够从一个初始值开始，找到一个标量函数的最小值。通常被称为无约束非线性优化\n",
    "\n",
    "* x = fminsearch(fun,x0) 从x0开始，找到函数fun中的局部最小值x，x0可以是标量，向量，矩阵。fun是一个函数句柄  \n",
    "\n",
    "* x = fminsearch(fun,x0,options) 以优化参数指定的结构最小化函数，可以用optimset函数定义这些参数。（见matlab help）\n",
    "\n",
    "* [x,fval] = fminsearch(...)返回在结果x出的目标函数的函数值\n",
    "\n",
    "* [x,fval,exitflag] = fminsearch(...) 返回exitflag值来表示fminsearch退出的条件：\n",
    "\t 1--函数找到结果x\n",
    "\t 0--函数最大功能评价次数达到，或者是迭代次数达到\n",
    "\t -1--算法由外部函数结束\n",
    "     \n",
    "* [x,fval,exitflag,output] = fminsearch(...) 返回一个结构输出output，包含最优化函数的信息：\n",
    "     output.algorithm 使用的优化算法\n",
    "\t output.funcCount 函式计算次数\n",
    "\t output.iterations 迭代次数\n",
    "\t output.message 退出信息\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化参数选项options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过optimset函数设置或改变这些参数。其中有的参数适用于所有的优化算法，有的则只适用于大型优化问题，另外一些则只适用于中型问题。\n",
    "\n",
    "首先描述适用于大型问题的选项。这仅仅是一个参考，因为使用大型问题算法有一些条件。对于fminunc（fminsearch）函数来说，必须提供梯度信息。\n",
    "\n",
    "> LargeScale – 当设为'on'时使用大型算法，若设为'off'则使用中型问题的算法。\n",
    "    \n",
    "**适用于大型和中型算法的参数：**\n",
    "* Diagnostics – 打印最小化函数的诊断信息。       \n",
    "\n",
    "* Display – 显示水平。选择'off'，不显示输出；选择'iter'，显示每一步迭代过程的输出；选择'final'，显示最终结果。打印最小化函数的诊断信息。      \n",
    "\n",
    "* GradObj – 用户定义的目标函数的梯度。对于大型问题此参数是必选的，对于中型问题则是可选项。     \n",
    "\n",
    "* MaxFunEvals – 函数评价的最大次数。     \n",
    "\n",
    "* MaxIter – 最大允许迭代次数。     \n",
    "\n",
    "* TolFun – 函数值的终止容限。     \n",
    "\n",
    "* TolX – x处的终止容限。\n",
    "\n",
    "**只用于大型算法的参数：**   \n",
    "* Hessian – 用户定义的目标函数的Hessian矩阵。\n",
    "\n",
    "* HessPattern –用于有限差分的Hessian矩阵的稀疏形式。若不方便求fun函数的稀疏Hessian矩阵H，可以通过用梯度的有限差分获得的H的稀疏结构（如非零值的位置等）来得到近似的Hessian矩阵H。若连矩阵的稀疏结构都不知道，则可以将HessPattern设为密集矩阵，在每一次迭代过程中，都将进行密集矩阵的有限差分近似（这是缺省设置）。这将非常麻烦，所以花一些力气得到Hessian矩阵的稀疏结构还是值得的。\n",
    "\n",
    "* MaxPCGIter – PCG迭代的最大次数。\n",
    "\n",
    "* PrecondBandWidth – PCG前处理的上带宽，缺省时为零。对于有些问题，增加带宽可以减少迭代次数。\n",
    "\n",
    "* TolPCG – PCG迭代的终止容限。\n",
    "\n",
    "* TypicalX – 典型x值。\n",
    "  \n",
    "**只用于中型算法的参数：**   \n",
    "* DerivativeCheck – 对用户提供的导数和有限差分求出的导数进行对比。\n",
    "\n",
    "* DiffMaxChange – 变量有限差分梯度的最大变化。\n",
    "\n",
    "* DiffMinChange - 变量有限差分梯度的最小变化。\n",
    "\n",
    "* LineSearchType – 一维搜索算法的选择。\n",
    "\n",
    "\n",
    "* exitflag:描述退出条件\n",
    "  * exitflag>0 表示目标函数收敛于解x处。\n",
    "  * exitflag=0 表示已经达到函数评价或迭代的最大次数。\n",
    "  * exitflag<0 表示目标函数不收敛。\n",
    "\n",
    "\n",
    "* output:该参数包含下列优化信息：\n",
    "  * output.iterations – 迭代次数。\n",
    "  * output.algorithm – 所采用的算法。\n",
    "  * output.funcCount – 函数评价次数。\n",
    "  * output.cgiterations – PCG迭代次数（只适用于大型规划问题）。\n",
    "  * output.stepsize – 最终步长的大小（只用于中型问题）。\n",
    "  * output.firstorderopt – 一阶优化的度量：解x处梯度的范数。\n",
    "\n",
    "\n",
    "* fminunc（fminsearch）为中型优化算法的搜索方向提供了4中算法，由options中的参数HessUpdate控制\n",
    "  * HessUpdate=‘bfgs’（默认值），为拟牛顿的BFGS法\n",
    "  * HessUpdate='dfp'为拟牛顿DFP法\n",
    "  * HessUpdate=‘steepdesc’最速下降法\n",
    "\n",
    "\n",
    "* fminunc（fminsearch）中为中型优化算法的步长一维搜索提供了两种算法，由options中参数LineSearchType控制\n",
    "  * LineSearchType='quadcubic'混合的二次和三次多项式插值\n",
    "  * LineSearchType=‘cubicpoly’三次多项式插值\n",
    "\n",
    "> 使用fminunc和fminsearch都可能会得到局部最优解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fminsearch函数的缺陷"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 依赖于初值\n",
    "2. 不同目标函数需要选择不同的算法\n",
    "3. 仅是局部最优算法，并不是全局最优\n",
    "4. 迭代法与解析解的速度不是一个量级"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fminsearch函数实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 求解 $z=(x+1)^2+(y-1)^2$ 的最小值**\n",
    "\n",
    "三维图像显示：\n",
    "<div align=center><img src=\"https://lei-picture.oss-cn-beijing.aliyuncs.com/img/20200428003742.png\" width=\"350\" ></div>\n",
    "\n",
    "\n",
    "可能不太好从图像里看出极值，这里再举一个例子：\n",
    "\n",
    "$z=-\\sqrt{(x+1)^2+(y-1)^2}$ 三维图 \n",
    "<div align=center><img src=\"https://lei-picture.oss-cn-beijing.aliyuncs.com/img/20200428003850.png\" width=\"350\" ></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```matlab\n",
    "% 1.编写目标函数\n",
    "function z = ObjectFunction(Beta)\n",
    "\tz = (Beta(1)+1)^2 + (Beta(2)-1)^2;\n",
    "\n",
    "% 2.初始参数设定\n",
    "maxsize         = 2000;         % 生成均匀随机数的个数(用于赋初值)\n",
    "REP\t\t\t    = 100;          % 若发散则继续进行迭代的次数\n",
    "nInitialVectors    = [maxsize, 2];    % 生成随机数向量\n",
    "MaxFunEvals    = 5000;         % 函数评价的最大次数\n",
    "MaxIter         = 5000;         % 允许迭代的最大次数\n",
    "options = optimset('LargeScale', 'off','HessUpdate', 'dfp','MaxFunEvals', ...\n",
    "MaxFunEvals, 'display', 'on', 'MaxIter', MaxIter, 'TolFun', 1e-6, 'TolX', 1e-6,'TolCon',10^-12);\n",
    "\n",
    "% 3.寻找最优初值\n",
    "initialTargetVectors = unifrnd(-5,5, nInitialVectors);\n",
    "RQfval = zeros(nInitialVectors(1), 1);\n",
    "for i = 1:nInitialVectors(1)\n",
    "    RQfval(i) = ObjectFunction(initialTargetVectors(i,:));\n",
    "end\n",
    "Results          = [RQfval, initialTargetVectors];\n",
    "SortedResults    = sortrows(Results,1);\n",
    "BestInitialCond  = SortedResults(1,2: size(Results,2));    \n",
    "\n",
    "% 4.迭代求出最优估计值Beta\n",
    "[Beta, fval exitflag] = fminsearch(' ObjectFunction ', BestInitialCond);\n",
    "for it = 1:REP\n",
    "if exitflag == 1, break, end\n",
    "[Beta, fval exitflag] = fminsearch(' ObjectFunction ', BestInitialCond);\n",
    "end\n",
    "if exitflag~=1, warning('警告：迭代并没有完成'), end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 求解 $\\ln L=-\\frac{1}{2} \\ln 2 \\pi-\\frac{1}{2} \\ln \\left[\\sigma_{\\varepsilon}^{2} /\\left(1-\\beta_{1}^{2}\\right)\\right]-\\frac{\\left[y_{1}-\\left(\\beta_{0} /\\left(1-\\beta_{1}\\right)\\right)\\right]^{2}}{2 \\sigma_{\\varepsilon}^{2} /\\left(1-\\beta_{1}^{2}\\right)}-\\frac{T-1}{2} \\ln 2 \\pi-\\frac{T-1}{2} \\ln \\sigma_{\\varepsilon}^{2}-\\sum_{t=2}^{T} \\frac{\\left(y_{t}-\\beta_{0}-\\beta_{1} y_{t-1}\\right)^{2}}{2 \\sigma_{\\varepsilon}^{2}}$ 的最大值**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```matlab\n",
    "%导入数据\n",
    "data = xlsread('./数据/hourse.xlsx');\n",
    "f1 = data(:,2); f2 = data(:,3); e = data(:,6);\n",
    "\n",
    "% 1.编写目标函数\n",
    "function lnL = ARlnL(Beta,Y)\n",
    "%Beta(1) = Beta0 ;Beta(2) = Beta1 ;Beta(3) = siga ;\n",
    "T = length(Y);\n",
    "lnL1 = -0.5*log(2*pi)-0.5*log(Beta(3)^2/(1-Beta(2)^2))-(Y(1)-(Beta(1)/(1-Beta(2))))^2/(2*Beta(3)^2/(1-Beta(2)^2))-0.5*log(2*pi)*(T-1)-0.5*(T-1)*log(Beta(3)^2);\n",
    "for i = 2:T\n",
    "    lnL2(i) = (Y(i)- Beta(1)-Beta(2)*Y(i-1))^2;\n",
    "end\n",
    "lnL = lnL1-0.5*sum(lnL2)/Beta(3)^2;\n",
    "lnL = -lnL;\n",
    "\n",
    "% 2.初始参数设定\n",
    "maxsize         = 2000;         % 生成均匀随机数的个数(用于赋初值)\n",
    "REP\t\t\t    = 100;          % 若发散则继续进行迭代的次数\n",
    "nInitialVectors    = [maxsize, 3];    % 生成随机数向量\n",
    "MaxFunEvals    = 5000;         % 函数评价的最大次数\n",
    "MaxIter         = 5000;         % 允许迭代的最大次数\n",
    "options = optimset('LargeScale', 'off','HessUpdate', 'dfp','MaxFunEvals', ...\n",
    "MaxFunEvals, 'display', 'on', 'MaxIter', MaxIter, 'TolFun', 1e-6, 'TolX', 1e-6,'TolCon',10^-12);\n",
    "\n",
    "% 3.寻找最优初值\n",
    "initialTargetVectors = unifrnd(-1,1, nInitialVectors);\n",
    "RQfval = zeros(nInitialVectors(1), 1);\n",
    "for i = 1:nInitialVectors(1)\n",
    "    RQfval(i) = ARlnL (initialTargetVectors(i,:), f1);\n",
    "end\n",
    "Results          = [RQfval, initialTargetVectors];\n",
    "SortedResults    = sortrows(Results,1);\n",
    "BestInitialCond  = SortedResults(1,2: size(Results,2));    \n",
    "\n",
    "% 4.迭代求出最优估计值Beta\n",
    "[Beta, fval exitflag] = fminsearch(' ARlnL ', BestInitialCond,options,f1);\n",
    "for it = 1:REP\n",
    "if exitflag == 1, break, end\n",
    "[Beta, fval exitflag] = fminsearch(' ARlnL ', BestInitialCond,options,f1);\n",
    "end\n",
    "if exitflag~=1, warning('警告：迭代并没有完成'), end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终结果为：\n",
    "\n",
    "|   1    |   2    |   3    |\n",
    "| :----: | :----: | :----: |\n",
    "| 0.1136 | 0.9793 | 1.8356 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
